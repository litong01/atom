#!/bin/bash

ColorOff='\033[0m'        # Text Reset
Black='\033[0;30m'        # Black
Red='\033[0;31m'          # Red
Green='\033[0;32m'        # Green
SCRIPTDIR=$(dirname $0)
APPNAME=$(basename "$0")
CURDIR=$(pwd)
WORKDIR=${WORKDIR:-/home/.kube}
COMPONENT="all"
NAMESPACE="${NAMESPACE:-astra-connector}"
DEPLOYMENT="${DEPLOYMENT:-neptune-controller-manager}"
TAG=${TAG:-latest}
SECRETNAME=${SECRETNAME:-regcred}
APITOKEN=${APITOKEN:-""}
CLUSTERNAME=${CLUSTERNAME:-"atom"}
CONNECTORCLUSTERNAME=${CLUSTERNAME:-"atom"}
K8SRELEASE=${K8SRELEASE:-"1.29.0"}
WORKERNODES=0

function printHelp() {
  echo ""
  echo "Usage:"
  echo "    ${APPNAME} <cmd> [options]"
  echo ""
  echo -e "Available commands:"
  echo "     up       - Start up astra network include cluster"
  echo "     down     - Remove all astra but k8s cluster"
  echo "     image    - Update image to the local repo"
  echo "     prepare  - Get cluster and image ready"
  echo "     deploy   - Deploy neptuen controller only"
  echo "     clean    - Remove all astra including k8s cluster"
  echo "     cleanall - Remove all astra including k8s cluster, proxy, local registry"
  echo "     update   - update this tool"
  echo "     refresh  - restart a deployment and its image"
  echo "     proxy    - make load balancer accessible locally"
  echo "     test     - run neptune tests"
  echo "     vtest    - verify test results"
  echo "     rmtest   - remove neptune tests"
  echo "     rstest   - test restore of a backup"
  echo "     make     - run atom make command"
  echo "     conn     - deploy connector"
  echo "     bash     - run bash interactively"
  echo "     log      - display logs"
  echo "     k9s      - run k9s tool"
  echo "     k        - run kubectl command"
  echo "     go       - run go command"
  echo "     gobuild  - run build native utility"
  echo "     build    - run build atom container images"
  echo ""
  echo -e "Parameters for ${Green}up, prepare${ColorOff} command:"
  echo "     -r|--k8srelease     - a k8s release to be used to create k8s cluster"
  echo -e "                           valid values are 1.21.14 ~ 1.29.0 default is ${Green}1.29.0${ColorOff}"
  echo "        --cluster-name   - name of the k8s cluster to be created"
  echo "        --work-nodes     - number of worker nodes, default 0"
  echo ""
  echo -e "Parameters for ${Green}refresh${ColorOff} command:"
  echo "     -d|--deployment-name  - a deployment name such as neptune-controller-manager"
  echo "     -n|--namespace        - the namespace where the deployment is, default astra-connector"
  echo ""
  echo -e "Parameters for ${Green}deploy${ColorOff} command:"
  echo "     -a|--astra-account-id - astra account id"
  echo "     -t|--apitoken         - astra api token"
  echo "     -s|--secretname       - image pulling secret name"
  echo "     --aliasip             - astra alias ip address"
  echo "     -c|--cloud-id         - cloud id"
  echo "     -i|--cluster-id       - cluster id"
  echo "     --connector-cluster-name  - name of the k8s cluster to be registered with astra"
  echo ""
}

function validateCMD() {
  cmd=$1
  allCommands=("up" "down" "image" "deploy" "clean" "cleanall" "prepare" "refresh" "make" "proxy" "log" "k9s" "gobuild" \
    "build" "update" "k8stool" "bash" "addon" "hostpath" "test" "rmtest" "vtest" "rstest" "testhostpath" "trident" "k" "go")
  ccmd=""
  for item in "${allCommands[@]}"; do
    if [[ "${cmd}" == "${item}" ]]; then
      ccmd="${cmd}"
      isValidCMD="true"
      break
    fi
  done
  if [[ -z "${ccmd}" ]]; then
    if [[ "${cmd}" != "-h" ]] && [[ "${cmd}" != "--help" ]] && [[ "${cmd}" != "" ]]; then
      echo ""
      echo -e "ERROR: ${Red}${cmd}${ColorOff} is not a supported command!"
      printHelp "${isValidCMD}"
      exit 1
    else
      printHelp "${isValidCMD}"
      exit 0
    fi
  fi
}

CMD=$1
shift
# This saves the rest of the command in case it is to pass along for make command
REST="$@"
# Validate the command
validateCMD "${CMD}"

# We will only handle command parameters if command was not make and not k8stool
if [[ "${CMD}" == "make" || "${CMD}" == "k8stool" || "${CMD}" == "bash" || "${CMD}" == "k" || "${CMD}" == "go" || "${CMD}" == "gobuild" ]]; then
  echo ""
else
  # Handling parameters
  while [[ $# -gt 0 ]]; do
    optkey="$1"
    case $optkey in
      -h|--help)
        printHelp "true"; exit 0;;
      --targetports)
        TARGETPORTS="$2";shift 2;;
      -n|--namespace)
        NAMESPACE="$2";shift 2;;
      -w|--workdir)
        WORKDIR="$2";shift 2;;
      -a|--astra-account-id)
        ACCOUNTID="$2";shift 2;;
      -d|--deployment-name)
        DEPLOYMENT="$2";shift 2;;
      -s|--secretname)
        SECRETNAME="$2";shift 2;;
      -t|--apitoken)
        APITOKEN="$2";shift 2;;
      --cluster-name)
        CLUSTERNAME="$2";shift 2;;
      --worker-nodes)
        WORKERNODES="$(($2+0))";shift 2;;
      -r|--k8s-release)
        K8SRELEASE="$2";shift 2;;
      --bridgeurl)
        BRIDGEURL="$2";shift 2;;
      --aliasip)
        ALIASIP="$2";shift 2;;
      -c|--cloudid)
        CLOUDID="$2";shift 2;;
      -i|--clusterid)
        CLUSTERID="$2";shift 2;;
      --connector-cluster-name)
        CONNECTORCLUSTERNAME="$2";shift 2;;
      *) # unknown option
        echo "parameter $1 is not supported"; exit 1;;
    esac
  done
fi

function setupContext() {
  # Always use the ${CLUSTERNAME}.yaml as the kubeconfig file
  export KUBECONFIG=${HOME}/.kube/${CLUSTERNAME}.yaml
}

function setupTrident() {
  set +e
  tridentns="trident"
  kubectl get namespace ${tridentns} >/dev/null 2>&1
  # namespace does not exist, create it
  if [[ $? == 1 ]]; then
    echo "Creating namespace ${tridentns}"
    set -e
    kubectl create ns ${tridentns}
  else
    echo "Namespace ${tridentns} already exists"
  fi
  set -e

  echo -e "${Green}Deploying trident onto kubernetes cluster...${ColorOff}"
  kubectl apply -f /home/trident/tridentorchestrator.yaml -n ${tridentns}
  echo "Trident CRDs are deployed"
  ARCH=$(uname -m) && if [[ "${ARCH}" == "aarch64" ]]; then ARCH=arm64; fi
  if [[ "${ARCH}" == "x86_64" ]]; then ARCH="amd64"; fi

  # Now make sure that the node using right architecture node label
  ARCH=$ARCH yq e '.spec.controllerPluginNodeSelector."kubernetes.io/arch" = env(ARCH)' -i /home/trident/tridentorchestrator_cr.yaml
  ARCH=$ARCH yq e '.spec.nodePluginNodeSelector."kubernetes.io/arch" = env(ARCH)' -i /home/trident/tridentorchestrator_cr.yaml

  # Deploy the orchestrator
  kubectl apply -f /home/trident/tridentorchestrator_cr.yaml -n ${tridentns}
}

function installStorageCRDs() {
  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.2.1/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml
  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.2.1/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml
  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.2.1/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml
  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.2.1/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml
  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.2.1/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml

  # We will also install trident crds to avoid an connector issue which assumes whatever
  # resource it looks for already have CRDs installed which is wrong. this can be removed
  # once connector fixes the problems.
  kubectl apply -f /home/addon/tridentcrds.yaml
}

function doADTUpdate() {
  adtimage=$(docker inspect atom --format "{{.Config.Image}}" 2>/dev/null || true)
  if [[ ! -z "${adtimage}" ]]; then
    if [[ ! -z $REGISTRY_USERID ]] && [[ ! -z $REGISTRY_TOKEN ]]; then
      if [[ -z $REGISTRY ]]; then
        echo ${REGISTRY_TOKEN} | docker login docker.io -u ${REGISTRY_USERID} --password-stdin >/dev/null 2>&1
      else
        echo ${REGISTRY_TOKEN} | docker login $REGISTRY -u ${REGISTRY_USERID} --password-stdin >/dev/null 2>&1
      fi
    fi
    # Pull the image
    docker pull "${adtimage}"
    # Try to clean up dangling images, use a variable to avoid none 0 return
    notuseval=$(docker rmi -f $(docker images -f "dangling=true") >/dev/null 2>&1 || true)
  fi
}

function loginToDockerHub() {
  if [[ ! -z "${GITHUB_ID}" ]] && [[ ! -z "${GITHUB_TOKEN}" ]]; then
    echo -e "${Green}Logging into docker hub...${ColorOff}"
    echo $DH_TOKEN | docker login -u $DH_ID --password-stdin docker.io >/dev/null 2>&1
  fi
}

function checkInVC() {
    if [[ ! -f "${CURDIR}/Makefile" ]]; then
      echo -e "${Red}You are not in NetApp Volume Controller root directory, quiting...${ColorOff}"
      exit 1
    fi
    projectName=$(cat Makefile | grep 'echo "NetApp Volume Controller"')
    if [[ -z "${projectName}" ]]; then
      echo -e "${Red}You are not in neptune root directory, quiting...${ColorOff}"
      exit 1
    fi
    chown $(id -u):$(id -g) $HOME/.kube
}

function checkEnvironmentVariables() {
  envs=(REGISTRY REGISTRY_USERID REGISTRY_TOKEN TAG PLATFORMS)
  for value in ${envs[@]}; do
    if [[ "${value}" == "REGISTRY_TOKEN" ]]; then
      echo -e "${Green}${value}${ColorOff}=************"
    else
      echo -e "${Green}${value}${ColorOff}=${!value}"
    fi
  done
  echo ""
}

generate_uuid() {
  local input_string="$1"
  local namespace_uuid="1b671a64-40d5-491e-99b0-da01ff1f3341" # Namespace UUID
  local hash

  # Generate SHA-1 hash of the namespace UUID concatenated with the input string
  hash=$(echo -n "$namespace_uuid$input_string" | openssl dgst -sha1)

  # Extract the hash value without the '(stdin)= ' prefix
  hash="${hash#*= }"

  # Format the hash as a UUID v5
  local time_low="${hash:0:8}"
  local time_mid="${hash:8:4}"
  local time_high_and_version="${hash:12:4}"
  local clock_seq_high_and_res_clock_seq_low="${hash:16:4}"
  local node="${hash:20:12}"

  # Set the version to 5 (SHA-1 Name-Based UUID)
  time_high_and_version=$(printf '%04x' "$((0x$time_high_and_version & 0x0fff | 0x5000))")

  # Set the variant to DCE 1.1, ISO/IEC 11578:1996
  clock_seq_high_and_res_clock_seq_low=$(printf '%04x' "$((0x$clock_seq_high_and_res_clock_seq_low & 0x3fff | 0x8000))")

  # Output the formatted UUID
  echo "$time_low-$time_mid-$time_high_and_version-$clock_seq_high_and_res_clock_seq_low-$node"
}


function downOperator() {
  echo -e "${Green}Uninstalling Neptune and Connector...${ColorOff}"
  echo -e "Target Kubernetes onto cluster: ${Green}${CLUSTERNAME}${ColorOff}"

  kubectl delete -f https://github.com/NetApp/astra-connector-operator/releases/latest/download/astraconnector_operator.yaml

  kubectl delete namespace astra-connector-operator astra-connector

  echo -e "${Green}Neptune has been removed!!!${ColorOff}"

}

function buildnvcclientcli() {
  echo -e "${Green}Building nvcclientcli...${ColorOff}"

  gocache=$(pwd)/.cache.d/arm64
  mkdir -p ${gocache}
  export GOPATH=${gocache}/pkg
  export GOCACHE=${gocache}
  # make sure that the binary is built with the right version
  export TAG=$(git rev-parse --short HEAD)
  export flags="-X quark.netapp.io/volume-controller/cmd/nvcclientcli/cmd.Version=${TAG} -extldflags -static"
  thecmd="go build -ldflags='${flags}' -o bin/anvcclientcli cmd/nvcclientcli/*.go"
  sh -c "${thecmd}"
}

function createCerts() {

  helm upgrade --install cert-gen	--namespace atom-quark \
		--set image.registry=${REGISTRY} \
		--set image.digest="" helm/charts/cert-gen

}

function deployAtom() {
  echo -e "${Green}Deploying Atom operator...${ColorOff}"
  TAG=$(git rev-parse --short HEAD)

  # Create two namespaces first
  kubectl create namespace atom-quark --dry-run=client -o yaml \
    | kubectl apply -f - > /dev/null 2>&1
  kubectl create namespace quark-system --dry-run=client -o yaml \
    | kubectl apply -f - > /dev/null 2>&1

  if [[ ! -f bin/anvcclientcli ]]; then
    buildnvcclientcli
  fi

  if [[ -z ${REGISTRY} ]]; then
    REGISTRY="kind-registry:5001"
  fi

  bin/anvcclientcli reconcile operator \
		-N atom-quark \
		-C /home/.kube/${CLUSTERNAME}.yaml -v ${TAG} \
		--override images.globalRegistry=${REGISTRY} \
		--override images.quarkop.digest= \
		--override images.imagePullPolicy=Always \
		--override containerParameters.debug=true \
		--override containerParameters.enableBucket=true \
		--override gcov.enabled=false \
		--override gcov.images.initContainer.registry=${REGISTRY}

  createCerts

  bin/anvcclientcli reconcile quarkCR \
	  -N atom-quark \
	  -C /home/.kube/${CLUSTERNAME}.yaml \
	  --override images.quark.tag=${TAG} \
	  --override images.ontap.tag=${TAG} \
	  --override quarkVersion=${TAG} \
	  --cloud gcp \
	  --override project.tenantProjectNumber=1047111997442 \
	  --variant internal \
	  --regional true \
	  --override debug=true \
    --override darkContent=false \
    --override featureFlags.enableGcp1p=true \
    --override featureFlags.disableDiskPacking=false \
    --override featureFlags.enableEtcdCostReduction=true \
    --override featureFlags.enableAzureTp=false \
    --override featureFlags.enableIntegratedBackup=false \
    --override featureFlags.enableGranularLiveness=true \
    --override featureFlags.enableThinProvisioningTuning=false \
    --override featureFlags.enableServiceMesh=false \
    --override featureFlags.rightSize=true \
    --override featureFlags.nodevol=true \
    --override featureFlags.enableSWIFTNetworking=false \
    --override devFeatureFlagsString="" \
    --override project.clusterName=tli-nvc-cluster \
    --override project.customerProjectNumber=tli \
    --override project.tenantProjectNumber="1047111997442" \
    --override controllerConfig.variant="internal" \
    --override controllerConfig.etcdClusterSize="3" \
    --override aksProject.azureCustomerSubscriptionId="" \
    --override aksProject.azureCustomerResourceGroup="tli-nvc" \
    --override logging.url=localhost \
    --override logging.output=stdout \
    --override volume.minPVCSize=1073741824 \
    --override volume.maxPVCSize=70368744177664 \
    --override volume.minDataPVCs=1 \
    --override featureFlags.nodevol=true \
    --override ilb.subnet=https://www.googleapis.com/compute/v1/projects/quark-dev-234914/regions/us-east1/subnetworks/quark-dev-producer-us-east1-ilb \
    --override ilb.protocol=BOTH \
    --override images.imagePullPolicy=Always \
    --override images.quark.registry=${REGISTRY} \
    --override images.ontap.registry=${REGISTRY} \
    --override images.external.registry=${REGISTRY} \
    --override images.quark.tag=${TAG} \
    --override images.quark.podrick.tag=${TAG} \
    --override images.quark.nvc.tag=${TAG} \
    --override images.quark.nsc.tag=${TAG} \
    --override images.quark.csc.tag=${TAG} \
    --override images.quark.nhc.tag=${TAG} \
    --override images.quark.nodeMonitor.tag=${TAG} \
    --override images.quark.nodevolWorker.tag=${TAG} \
    --override images.quark.nodevolController.tag=${TAG} \
    --override images.quark.svcmeshctrl.tag=${TAG} \
    --override images.ontap.tag= \
    --override images.ontap.dmap.tag=${TAG} \
    --override images.ontap.secd.tag=${TAG} \
    --override images.ontap.ccpd.tag=${TAG} \
    --override images.quark.snc.tag=${TAG}

}

function deployOperator() {
  echo -e "Deploying Astra Connector Operator on cluster: ${Green}${CLUSTERNAME}${ColorOff}..."

  kubectl apply -f https://github.com/NetApp/astra-connector-operator/releases/latest/download/astraconnector_operator.yaml > /dev/null 2>&1
  echo -e "${Green}Waiting for Astra Connector Operator to be ready..."
  waitforpod 'astra-connector-operator' 'control-plane=controller-manager'

  echo -e "${Green}Astra Connector Operator is ready!!!${ColorOff}"
  echo -e "${Green}You may now deploy an Astra Connector resource.${ColorOff}"
}

function loginToRepo() {
  reg=$1
  if [[ -f ${WORKDIR}/login ]]; then
    keyPair=$(cat ${WORKDIR}/login)
    id=$(echo $keyPair | cut -d ":" -f 1)
    pw=$(echo $keyPair | cut -d ":" -f 2)
    echo ${pw} | docker login ${reg} -u ${id} --password-stdin >/dev/null 2>&1 || true
  fi
}

function doATOMImageBuild() {
  echo -e "${Green}Building ATOM images...${ColorOff}"

  makeTarget=(build-csc build-nvc build-nc build-nw build-podrick build-quarkha \
    build-nhc build-nm build-quarkop build-nsc build-snc)

} 

function extraImage() {
  # This is to build the extra images
  imgFullname="certs-gen:v0.1.78"
  remoteImage="us.gcr.io/netapp-hcl/${imgFullname}"
  localImage="localhost:5001/${imgFullname}"
  echo -e "${Green}Processing image ${imgFullname}${ColorOff}"
  img=$(docker image ls ${localImage} --quiet)
  if [[ -z ${img} ]]; then
    img=$(docker image ls ${remoteImage} --quiet)
    if [[ -z ${img} ]]; then
      echo -e "${Green}Image ${remoteImage} not exists, pull that image first${ColorOff}"
      exit 1
    else
      docker tag ${remoteImage} ${localImage}
    fi
  fi
  docker push "${localImage}"
}

function doATOMImage() {
  # The assumption is that this repo has all necessary atom needed images
  echo -e "${Green}Setting up ATOM images...${ColorOff}"

  imageTag=$(git rev-parse --short HEAD)
  if [[ -z ${imageTag} ]]; then
    echo -e "${Red}Could not get the image tag${ColorOff}"
    return
  fi
  localrepo="localhost:5001"

  filecontent=$(cat /home/addon/atom-images.list)
  allimages=($(echo $filecontent))

  loggedIn="false"
  for img in ${allimages[@]}; do
    img=$(echo "${img%:}")  # this is to remove the possible trailing colon
    if [[ -z "${img}" ]]; then
      continue
    fi
    # Get the fullname of the image
    fullname=$(echo $img | cut -d ":" -f 1)
    # Get the name of the image
    name=$(echo ${fullname##*/})
    #handle the tag
    tag=$(echo $img | cut -d ":" -f 2)
    if [[ -z ${tag} ]] || [[ "${tag}" == "${fullname}" ]]; then
      tag=${imageTag}
    fi
    echo -e "${Green}Processing image ${fullname}:${tag}${ColorOff}"
    # check if the image is already available locally
    localImg=$(docker image ls ${localrepo}/${fullname}:${tag} --quiet)
    if [[ -z "${localImg}" ]]; then
      echo -e "${Green}Image ${fullname}:${tag} is not available locally, run make IMAGE_REPO=localhost:5001 build-components${ColorOff}"
    fi
    docker push ${localrepo}/${fullname}:${tag}
  done
  extraImage
}

function refreshDeployment() {
  if [[ "${DEPLOYMENT}" == "" ]]; then
    DEPLOYMENT="neptune-controller-manager"
  fi

  OLDIFS=$IFS
  dep=$(kubectl get -n ${NAMESPACE} deployment "${DEPLOYMENT}" -o \
    jsonpath='{.spec.template.spec.containers[*].image}' 2>/dev/null||true)
  if [[ "${dep}" == "" ]]; then
    echo -e "${Red}Could not find the deployment ${DEPLOYMENT}${ColorOff}"
    echo -e "${Green}If the deployment was not installed in astra-connector, you can use -n to specify${ColorOff}"
    IFS=$OLDIFS
    exit 1
  else
    # Now need to get the tag and the actual image name
    for anImage in ${dep[@]}; do
      IFS=$'/' parts=($(echo "$anImage"|rev))
      imagename=$(echo "${parts[0]}"|rev)
      IFS=$OLDIFS
      if [[ "${imagename}" != "" ]] && [[ "${anImage}" =~ "kind-registry:5001" ]]; then
          echo -e "Ready to update ${Green}${imagename}${ColorOff}"
          # astra image, push the newer image
          docker tag ${imagename} localhost:5001/${imagename}
          docker push localhost:5001/${imagename}
          # Remove the cached image
          docker exec ${CLUSTERNAME}-control-plane crictl rmi "${anImage}" > /dev/null 2>&1 || true
      fi
    done
    kubectl rollout restart -n ${NAMESPACE} deployment ${DEPLOYMENT}
  fi
}

function testHostpathSnapshot() {
  testns="testhostpath"
  # Create a namespace to test hostpath snapshot
  kubectl create namespace ${testns} --dry-run=client -o yaml \
    | kubectl apply -f -

  # Create persistent volume
  kubectl apply -n ${testns} -f /home/hostpath/test/csi-pvc.yaml
  sleep 1
  # Check pvc
  name=$(kubectl get -n ${testns} pvc -o jsonpath --template={.items[0].metadata.name})
  if [[ "${name}" == "csi-pvc" ]]; then
    echo -e "${Green}pvc created successfully${ColorOff}"
  else
    echo -e "${Red}pvc creation failed${ColorOff}"
  fi

  # Create volume snapshot
  kubectl apply -n ${testns} -f /home/hostpath/test/csi-snapshot-v1.yaml
  sleep 1

  # Check volume snapshot
  name=$(kubectl get -n ${testns} volumesnapshot -o jsonpath --template={.items[0].metadata.name})
  if [[ "${name}" == "new-snapshot-demo" ]]; then
    echo -e "${Green}snapshot created successfully${ColorOff}"
  else
    echo -e "${Red}snapshot creation failed${ColorOff}"
  fi

  # Check volume snapshot content
  name=$(kubectl get -n ${testns} volumesnapshotcontent -o jsonpath --template={.items[0].status.readyToUse})
  if [[ "${name}" == "true" ]]; then
    echo -e "${Green}snapshot content is ready${ColorOff}"
  else
    echo -e "${Red}snapshot content was not ready${ColorOff}"
  fi

  # Now restore the snapshot
  kubectl apply -n ${testns} -f /home/hostpath/test/csi-restore.yaml
  sleep 1
  # There should be two pvcs
  # Use this to get all the names
  #    name=$(kubectl get -n ${testns} pvc -o jsonpath --template={.items[*].metadata.name})
  # Get the 2nd pvc name
  name=$(kubectl get -n ${testns} pvc -o jsonpath --template={.items[1].metadata.name})
  if [[ ! -z ${name} ]]; then
    echo -e "${Green}pvc restore was successful${ColorOff}"
  else
    echo -e "${Red}pvc restore failed${ColorOff}"
  fi
}

function setupHostpath() {

  csins="hostpath"
  # Create a namespace to test hostpath snapshot
  kubectl create namespace ${csins} --dry-run=client -o yaml \
    | kubectl apply -f -

  /home/hostpath/deploy.sh

  # Create storage class
  kubectl apply -f /home/hostpath/storageclass/csi-storageclass.yaml

  # Set the standard not to be the default sc
  kubectl patch storageclass standard --type=merge -p \
   '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'

  # Set the hostpath to be the default
  kubectl patch storageclass csi-hostpath-sc --type=merge -p \
   '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

  # Set the default snapshot sc for volumesnapshot class
  kubectl patch volumesnapshotclass csi-hostpath-snapclass --type=merge -p \
   '{"metadata": {"annotations":{"snapshot.storage.kubernetes.io/is-default-class":"true"}}}'
}

function deployVault() {
  # This section deploys hashcorp vault onto the cluster
  echo -e "${Green}Deploying Hashicorp Vault...${ColorOff}"
  helm repo add hashicorp https://helm.releases.hashicorp.com >/dev/null 2>&1

  kubectl create namespace vault --dry-run=client -o yaml \
    | kubectl apply -f -

  # Try to uninstall it if there is one already there
  helm uninstall vault -n vault >/dev/nul 2>&1 || true
  sleep 3
  # Try to remove the pvc if there is one there
  kubectl delete pvc -n vault data-vault-0 >/dev/nul 2>&1 || true
  sleep 3

  # Try to install
  helm install vault hashicorp/vault --namespace vault --set ui.enable=true >/dev/null 2>&1 || true
  # Wait for vault to be ready to initialize
  while : ; do
    isready=$(kubectl logs -n vault vault-0 2>/dev/null | grep 'seal configuration missing, not initialized' | head -n1)
    if [[ ! -z $isready ]]; then
      break
    fi
    echo 'Waiting for vault to be available...'
    sleep 3
  done

  kubectl exec -ti -n vault statefulset/vault -- vault operator init --format json > /home/work/atom/vault.key.json
  allkeys=($(cat /home/work/atom/vault.key.json | jq -r '.unseal_keys_b64 | join(" ")'))
  for key in ${allkeys[@]}; do
    kubectl exec -ti -n vault statefulset/vault -- vault operator unseal ${key} >/dev/null 2>&1
  done
  kubectl apply -n vault -f /home/addon/vault-lb.yaml

  # Get root token
  # roottoken=$(cat /home/work/atom/vault.key.json | jq -r '.root_token')
  # Here we use the root token login first, then enable username password login,then create a admin user with root access
  # kubectl exec -n vault statefulset/vault -- \
  #  /bin/sh -c "echo ${roottoken} | vault login -; vault auth enable userpass; vault write auth/userpass/users/admin policies=root password=admin;"

}

function deployMinio() {
  echo -e "${Green}Deploying MinIO...${ColorOff}"
  # This section setup minio onto the cluster
  kubectl apply -n ${NAMESPACE} -f /home/addon/minio-dev.yaml
  kubectl apply -n ${NAMESPACE} -f /home/addon/minio-lb.yaml

  # Wait for minio to be ready
  echo -e "${Green}Waiting for Minio to be ready...${ColorOff}"
  showTimer 'minio' &
  while : ; do
    isready=$(kubectl logs -n ${NAMESPACE} minio 2>/dev/null | grep '1 Online, 0 Offline' | head -n1)
    if [[ ! -z $isready ]]; then
      break
    fi
    sleep 3
  done
  stopTimer 'minio'

  # Now we setup alias named minio and create bucket for test using the default userid and password
  kubectl exec -ti -n ${NAMESPACE} pod/minio \
    -- mc config host add --insecure minio https://minio.astra-connector.svc:9000 minioadmin minioadmin

  # Create the test app bucket
  kubectl exec -ti -n ${NAMESPACE} pod/minio \
    -- mc mb --insecure minio/minio-bucket >/dev/nul 2>&1 || true
}

function doAddon() {
  deployMinio
  # Now setup the pathecho for test post summary
  kubectl -n ${NAMESPACE} apply -f /home/addon/pathecho-tls.yaml
}

function proxy() {
  ${SCRIPTDIR}/k8stool proxy --cluster-name ${CLUSTERNAME}
}

function isClusterOCP() {
  res=$(kubectl get crd securitycontextconstraints.security.openshift.io --no-headers 2>/dev/null)
  echo ${res}
}

function setupTest() {
  workloadns=$1
  neptunens=$2
  targetns=$3

  echo -e "${Green}Creating workload namespace ${workloadns}...${ColorOff}"
  # Create a namespace for the workload to run
  kubectl create namespace ${workloadns} --dry-run=client -o yaml \
    | kubectl apply -f -

  # Now deploy the workload
  echo -e "${Green}Deploying workload...${ColorOff}"
  isOCP=$(isClusterOCP) # empty means not OCP, non empty means OCP
  if [[ -z ${isOCP} ]]; then
    kubectl apply -n ${workloadns} -f /home/examples/workload.yaml
  else
    ns="${workloadns}" yq 'select(.metadata.name == "test-scc-binding").subjects[0].namespace = env(ns)' -i /home/examples/ocpworkload.yaml
    kubectl apply -n ${workloadns} -f /home/examples/ocpworkload.yaml
  fi

  # Now create a namespace to hold all the neptune resources
  echo -e "${Green}Creating namespace ${neptunens} for neptune resources...${ColorOff}"
  kubectl create namespace ${neptunens} --dry-run=client -o yaml \
    | kubectl apply -f -

  allfiles=(app-vault.yaml app.yaml snapshot.yaml backup.yaml resoucesummaryupload.yaml)
  # fixup the app namespace attribute
  echo -e "${Green}Creating app for namespace ${workloadns}...${ColorOff}"
  ns="${workloadns}" yq '.spec.includedNamespaces[0].namespace = env(ns)' \
    -i /home/examples/app.yaml

  for afile in ${allfiles[@]}; do
    echo -e "${Green}Creating ${afile} in namespace ${neptunens}...${ColorOff}"
    kubectl apply -n ${neptunens} -f /home/examples/${afile}
  done
}

function removeTest() {
  workloadns=$1
  neptunens=$2

  # Now remove the workload
  isOCP=$(isClusterOCP) # empty means not OCP, non empty means OCP
  if [[ -z ${isOCP} ]]; then
    kubectl delete -n ${workloadns} -f /home/examples/workload.yaml
  else
    ns="${workloadns}" yq 'select(.metadata.name == "test-scc-binding").subjects[0].namespace = env(ns)' -i /home/examples/ocpworkload.yaml
    kubectl delete -n ${workloadns} -f /home/examples/ocpworkload.yaml
  fi
  # delete the namespace for the workload
  kubectl delete namespace ${workloadns} >/dev/null 2>&1

  allfiles=(resoucesummaryupload.yaml backup.yaml snapshot.yaml app.yaml app-vault.yaml)
  for afile in ${allfiles[@]}; do
    kubectl delete -n ${neptunens} -f /home/examples/${afile} || true
  done

}

function verifyTest() {
  workloadns=$1
  neptunens=$2

  source /home/bin/verify
  verifyTests "${workloadns}" "${neptunens}"
}

function restoreTest() {
  workloadns=$1
  neptunens=$2
  targetns=$3
  backupname="test-backup1"

  appArchivePath=$(kubectl -n ${neptunens} get backups -o jsonpath='{range .items[?(@.metadata.name == "'$backupname'")]}{.status.appArchivePath}{end}')
  if [[ -z "${appArchivePath}" ]]; then
    echo -e "${Red}Cannot find the backup ${backupname}${ColorOff}"
    exit 1
  fi

  SNS=$appArchivePath yq e '.spec.appArchivePath = env(SNS)' -i /home/examples/backuprestore.yaml
  SNS=$workloadns yq e '.spec.namespaceMapping[0].source = env(SNS)' -i /home/examples/backuprestore.yaml
  SNS=$targetns yq e '.spec.namespaceMapping[0].target = env(SNS)' -i /home/examples/backuprestore.yaml

  echo -e "${Green}Ready to create the restore CR${ColorOff}"
  cat /home/examples/backuprestore.yaml
  # Now create this backup restore CR
  kubectl apply -n ${neptunens} -f /home/examples/backuprestore.yaml

}

function setLogCmd() {

myat='$@'
LOGCMDS=$(cat << EOF

nlog() {
  jq -cr --raw-input 'import "logs" as logs; logs::nlog'
}

nlogs() {
  stern --tail 100 -o json deploy/neptune-controller-manager -n astra-connector $myat | jq -cr --raw-input 'import "logs" as logs; logs::sternlog(logs::nlog)'
}
EOF
)

}

function setupIstio() {
  echo -e "${Green}Setting up Istio...${ColorOff}"
  # This section setup istio onto the cluster
  kubectl create namespace istio-system --dry-run=client -o yaml \
    | kubectl apply -f -

  # Install istio
  istioctl install --set profile=ambient --skip-confirmation

  # Install kubernetes gateway API CRDs
  kubectl get crd gateways.gateway.networking.k8s.io &> /dev/null || \
  { kubectl kustomize "github.com/kubernetes-sigs/gateway-api/config/crd/experimental?ref=v1.1.0" | kubectl apply -f -; }

  echo -e "${Green}Istio has been setup!!!${ColorOff}"
}

#=================Start main process=========================

set -e
checkEnvironmentVariables
setupContext
# cluster processes
if [[ "${CMD}" == "up" ]]; then
  echo ""
  # checkInVC
  source /home/bin/utils

  echo -e "${Green}Setting up kubernetes cluster...${ColorOff}"
  ${SCRIPTDIR}/k8stool clusters --workdir "${WORKDIR}" --cluster-name "${CLUSTERNAME}" --k8s-release ${K8SRELEASE}
  
  installStorageCRDs

  # ${SCRIPTDIR}/k8stool cert --namespace ${NAMESPACE} --workdir "${WORKDIR}" --cluster-name "${CLUSTERNAME}"
  
  # doATOMImage

  # Setup hostpath csi driver without test
  # doAddon
  setupHostpath

  # deployOperator

elif [[ "${CMD}" == "prepare" ]]; then
  echo ""
  # checkInVC
  source /home/bin/utils

  echo ""
  echo -e "${Green}Setting up kubernetes cluster...${ColorOff}"
  ${SCRIPTDIR}/k8stool clusters --workdir "${WORKDIR}" --cluster-name ${CLUSTERNAME} --k8s-release ${K8SRELEASE} --worker-nodes ${WORKERNODES}

  installStorageCRDs

  # echo -e "${Green}Create traefik certificate${ColorOff}"
  ${SCRIPTDIR}/k8stool cert --namespace ${NAMESPACE} --workdir "${WORKDIR}" --cluster-name "${CLUSTERNAME}"

  setupIstio

  echo -e "${Green}Ready to deploy ATOM, you may run deploy command${ColorOff}"

elif [[ "${CMD}" == "down" ]]; then
  echo ""
  echo -e "${Green}Removing Neptune CRDs and Controllers...${ColorOff}"
  checkInVC

  downOperator

elif [[ "${CMD}" == "clean" ]]; then
  echo -e "${Green}Removing Neptune and cluster...${ColorOff}"
  ${SCRIPTDIR}/k8stool cluster -d --cluster-name ${CLUSTERNAME}
  rm -rf ${WORKDIR}/${CLUSTERNAME}.yaml
  rm -rf ${WORKDIR}/${CLUSTERNAME}

elif [[ "${CMD}" == "cleanall" ]]; then
  echo -e "${Green}Removing everything...${ColorOff}"
  ${SCRIPTDIR}/k8stool cluster -d --cluster-name ${CLUSTERNAME}
  rm -rf ${HOME}/.kube/${CLUSTERNAME}.yaml
  rm -rf ${WORKDIR}/${CLUSTERNAME}

  # Remove the proxy and the local registry as well
  docker rm -f "kubeproxy-${CLUSTERNAME}" &> /dev/null || true
  docker rm -f kind-registry &> /dev/null || true
  # Need to clean up docker volumes
  docker volume prune -f &> /dev/null || true

elif [[ "${CMD}" == "deploy" ]]; then
  checkInVC
  source /home/bin/utils
  deployAtom


elif [[ "${CMD}" == "refresh" ]]; then
  checkInVC
  refreshDeployment

elif [[ "${CMD}" == "image" ]]; then
  doATOMImage

elif [[ "${CMD}" == "update" ]]; then
  doADTUpdate

elif [[ "${CMD}" == "addon" ]]; then
  doAddon

elif [[ "${CMD}" == "hostpath" ]]; then
  setupHostpath

elif [[ "${CMD}" == "testhostpath" ]]; then
  testHostpathSnapshot

elif [[ "${CMD}" == "test" ]]; then
  setupTest test1 astra-connector

elif [[ "${CMD}" == "vtest" ]]; then
  verifyTest test1 astra-connector

elif [[ "${CMD}" == "rmtest" ]]; then
  removeTest test1 astra-connector

elif [[ "${CMD}" == "rstest" ]]; then
  restoreTest test1 astra-connector test2

elif [[ "${CMD}" == "proxy" ]]; then
  proxy

elif [[ "${CMD}" == "make" ]]; then
  checkInVC

  # We are ready to execute,
  make ${REST}

elif [[ "${CMD}" == "k" ]]; then

  kubectl ${REST}

elif [[ "${CMD}" == "go" ]]; then

  gocache=$(pwd)/.cache.d/arm64
  mkdir -p ${gocache}
  export GOPATH=${gocache}/pkg
  export GOCACHE=${gocache}
  go ${REST}

elif [[ "${CMD}" == "gobuild" ]]; then

  # setup both GOPATH and GOCACHE first to make sure that we can
  # always build things faster
  gocache=$(pwd)/.cache.d/arm64
  mkdir -p ${gocache}
  export GOPATH=${gocache}/pkg
  export GOCACHE=${gocache}
  # make sure that the binary is built with the right version
  export TAG=$(git rev-parse --short HEAD)
  export flags="-X quark.netapp.io/volume-controller/cmd/nvcclientcli/cmd.Version=${TAG} -extldflags -static"
  thecmd="go build -ldflags='${flags}' ${REST}"
  sh -c "${thecmd}"

elif [[ "${CMD}" == "log" ]]; then
  if [[ -z $DEPLOYMENT ]]; then
    DEPLOYMENT="neptune-controller-manager"
  fi
  stern --tail 100 -o json deploy/$DEPLOYMENT -n $NAMESPACE | \
    jq -cr --raw-input 'import "logs" as logs; logs::sternlog(logs::nlog)'

elif [[ "${CMD}" == "k9s" ]]; then

  k9s

elif [[ "${CMD}" == "trident" ]]; then

  setupTrident

elif [[ "${CMD}" == "bash" ]]; then

  echo 'source /etc/bash/bashrc' > $HOME/.bashrc
  echo 'alias k=kubectl' >> $HOME/.bashrc
  echo 'alias d=docker' >> $HOME/.bashrc
  echo 'source <(kubectl completion bash)' >> $HOME/.bashrc
  echo 'complete -o default -F __start_kubectl k' >> $HOME/.bashrc
  echo 'export HISTCONTROL=ignoreboth:erasedups' >> $HOME/.bashrc

  setLogCmd
  # The following code is to use treys log tool
  echo "${LOGCMDS}" >> $HOME/.bashrc

  echo -e "${Green}Entering bash session...${ColorOff}"
  bash --rcfile $HOME/.bashrc
fi
#=================End main process=========================
