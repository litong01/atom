#!/bin/bash

ColorOff='\033[0m'        # Text Reset
Black='\033[0;30m'        # Black
Red='\033[0;31m'          # Red
Green='\033[0;32m'        # Green
SCRIPTDIR=$(dirname $0)
APPNAME=$(basename "$0")
TYPE="acc"
CURDIR=$(pwd)
WORKDIR=${WORKDIR:-/tmp/astra3}
COMPONENT="all"
NAMESPACE="${NAMESPACE:-neptune-system}"
DEPLOYMENT="${DEPLOYMENT:-neptune-controller-manager}"
IMAGE_REPOSITORY=${IMAGE_REPOSITORY:-docker.repo.eng.netapp.com/globalcicd/astra}
IMAGENAME=""
TAG=${TAG:-latest}

function printHelp() {
  echo ""
  echo "Usage:"
  echo "    ${APPNAME} <cmd> [options]"
  echo ""
  echo -e "Available commands:"
  echo "     up       - Start up astra network include cluster"
  echo "     down     - Remove all astra but k8s cluster"
  echo "     image    - Update image to the local repo"
  echo "     prepare  - Get cluster and image ready"
  echo "     deploy   - Deploy astra components"
  echo "     clean    - Remove all astra including k8s cluster"
  echo "     cleanall - Remove all astra including k8s cluster, proxy, local registry"
  echo "     update   - update this tool"
  echo "     trident  - Set up trident"
  echo "     refresh  - restart a deployment and its image"
  echo "     test     - run neptune tests"
  echo "     rmtest   - remove neptune tests"
  echo "     make     - run neptune make command"
  echo "     bash     - run bash interactively"
  echo ""
  echo -e "Parameters for ${Green}refresh${ColorOff} command:"
  echo "     -d|--deployment-name  - a deployment name such as neptune-controller-manager"
  echo "     -n|--namespace        - the namespace where the deployment is, default neptune-system"
  echo ""
}

function validateCMD() {
  cmd=$1
  allCommands=("up" "down" "image" "deploy" "clean" "cleanall" "prepare" "refresh" "make" \
    "update" "k8stool" "bash" "addon" "trident" "hostpath" "test" "rmtest" "testhostpath")
  ccmd=""
  for item in "${allCommands[@]}"; do
    if [[ "${cmd}" == "${item}" ]]; then
      ccmd="${cmd}"
      isValidCMD="true"
      break
    fi
  done
  if [[ -z "${ccmd}" ]]; then
    if [[ "${cmd}" != "-h" ]] && [[ "${cmd}" != "--help" ]] && [[ "${cmd}" != "" ]]; then
      echo ""
      echo -e "ERROR: ${Red}${cmd}${ColorOff} is not a supported command!"
      printHelp "${isValidCMD}"
      exit 1
    else
      printHelp "${isValidCMD}"
      exit 0
    fi
  fi
}

CMD=$1
shift
# This saves the rest of the command in case it is to pass along for make command
REST="$@"
# Validate the command
validateCMD "${CMD}"

# We will only handle command parameters if command was not make and not k8stool
if [[ "${CMD}" == "make" || "${CMD}" == "k8stool" || "${CMD}" == "bash" ]]; then
  echo ""
else
  # Handling parameters
  while [[ $# -gt 0 ]]; do
    optkey="$1"
    case $optkey in
      -h|--help)
        printHelp "true"; exit 0;;
      --context)
        CONTEXT="$2";shift 2;;
      --targetports)
        TARGETPORTS="$2";shift 2;;
      -n|--namespace)
        NAMESPACE="$2";shift 2;;
      -w|--workdir)
        WORKDIR="$2";shift 2;;
      -a|--astra-name)
        COMPONENT="$2";shift 2;;
      -d|--deployment-name)
        DEPLOYMENT="$2";shift 2;;
      -i|--imagename)
        IMAGENAME="$2";shift 2;;
      -r|--releaseimage)
        RELEASEIMAGE="$2";shift 2;;
      -t|--type)
        TYPE="$2";shift 2;;
      *) # unknown option
        echo "parameter $1 is not supported"; exit 1;;
    esac
  done
fi


function validateContext() {
  # Context was not set, trying to use current context
  if [[ -z "${CONTEXT}" ]]; then
    # Get current context
    ctx=$(kubectl config current-context 2>/dev/null || true)
  else # Context was set, verify it exists in the kubernetes context
    ctx=$(kubectl config get-contexts "${CONTEXT}" -o=name 2>/dev/null || true)
  fi

  if [[ -z "${ctx}" ]]; then
    echo -e "${Red}No kubernetes context available, cannot continue${ColorOff}"
    exit 1
  fi

  CONTEXT="${ctx}"
}

function installStorageCRDs() {
  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.2.1/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml
  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.2.1/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml
  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.2.1/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml
  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.2.1/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml
  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.2.1/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml
}


function doADTUpdate() {
  adtimage=$(docker inspect astra3 --format "{{.Config.Image}}" 2>/dev/null || true)
  if [[ ! -z "${adtimage}" ]]; then
    # Using the github token and id to login
    echo $GITHUB_TOKEN | docker login -u ${GITHUB_ID} --password-stdin ghcr.io >/dev/null 2>&1
    # Pull the image
    docker pull "${adtimage}"
    docker pull ghcr.io/netapp-polaris/polaris/astra/acc-operator:astradt
    docker tag ghcr.io/netapp-polaris/polaris/astra/acc-operator:astradt acc-operator:astradt
    # Try to clean up dangling images, use a variable to avoid none 0 return
    notuseval=$(docker rmi -f $(docker images -f "dangling=true") >/dev/null 2>&1 || true)
  fi
}

function loginToDockerHub() {
  if [[ ! -z "${GITHUB_ID}" ]] && [[ ! -z "${GITHUB_TOKEN}" ]]; then
    echo -e "${Green}Logging into docker hub...${ColorOff}"
    echo $DH_TOKEN | docker login -u $DH_ID --password-stdin docker.io >/dev/null 2>&1
  fi
}

function checkInNeptune() {
    projectName=$(cat PROJECT | yq '.projectName')
    if [[ ! -f "${CURDIR}/Makefile" ]] || [[ "${projectName}" != "neptune" ]]; then
      echo -e "${Red}You are not in neptune root directory, quiting...${ColorOff}"
      exit 1
    fi
}

function checkEnvironmentVariables() {
  envs=(REGISTRY REGISTRY_USERID REGISTRY_TOKEN TAG PLATFORMS)
  for value in ${envs[@]}; do
    if [[ "${value}" == "REGISTRY_TOKEN" ]]; then
      echo -e "${Green}${value}${ColorOff}=************"
    else
      echo -e "${Green}${value}${ColorOff}=${!value}"
    fi
  done
  echo ""
}

function wait_for_pod() {
  namespace=$1
  podlabel=$2
  condition=$3
  while : ; do
    waitresult=$(kubectl wait pod --context ${CONTEXT} -n ${namespace} --for=condition=${condition} \
      -l "app.kubernetes.io/name in (${podlabel})" --timeout=60s 2>/dev/null || true)
    # If the wait returns, it can only be either timed out or condition met. We check if there is
    # timedout, we continue to wait. Since the timeout is an error, and it goes to /dev/null, only
    # condition met will have anything in waitresult, in any other condition, the waitresult is empty.
    if [[ -z "${waitresult}" ]]; then
      currentTime=$(date +%s)
      elapsed=$(( currentTime - startTime ))
      echo -ne "\033[0K\r"
      echo -ne "   Time elapsed: ${Green}${elapsed}${ColorOff} seconds\033[0K"
    else
       break
    fi
  done
  echo ""
  echo -e "${Green}AC pods are ready now${ColorOff}"
}

function deployNeptune() {
  echo -e "${Green}Deploying Neptune...${ColorOff}"
  IMG="kind-registry:5001/controller:${TAG}" REGISTRY="kind-registry:5001" TAG=${TAG} make deploy

  echo -e "${Green}Neptune is ready!!!${ColorOff}"
}

function doNeptuneImage() {
  echo -e "${Green}Setting up Neptune Manager image...${ColorOff}"
  allimages=(controller exechookjob resourcebackup resourcedelete resourcerestore resourcesummaryupload)
  tag=${TAG:-latest}
  for img in ${allimages[@]}; do
    localImg=$(docker image ls ${img}:${tag} --quiet)
    if [[ -z "${localImg}" ]]; then
      echo -e "${Red}${img}:${tag} local image does not exist, do a docker build first${ColorOff}"
      continue
    fi
    docker tag ${img}:${tag} localhost:5001/${img}:${tag}
    docker push localhost:5001/${img}:${tag}
  done
}

function refreshDeployment() {
  if [[ "${DEPLOYMENT}" == "" ]]; then
    DEPLOYMENT="neptune-controller-manager"
  fi

  OLDIFS=$IFS
  dep=$(kubectl --context ${CONTEXT} get -n ${NAMESPACE} deployment "${DEPLOYMENT}" -o \
    jsonpath='{.spec.template.spec.containers[*].image}' 2>/dev/null||true)
  if [[ "${dep}" == "" ]]; then
    echo -e "${Red}Could not find the deployment ${DEPLOYMENT}${ColorOff}"
    echo -e "${Green}If the deployment was not installed in neptune-system, you can use -n to specify${ColorOff}"
    IFS=$OLDIFS
    exit 1
  else
    # Now need to get the tag and the actual image name
    for anImage in ${dep[@]}; do
      IFS=$'/' parts=($(echo "$anImage"|rev))
      imagename=$(echo "${parts[0]}"|rev)
      IFS=$OLDIFS
      if [[ "${imagename}" != "" ]] && [[ "${anImage}" =~ "kind-registry:5001" ]]; then
          echo -e "Ready to update ${Green}${imagename}${ColorOff}"
          # astra image, push the newer image
          ${SCRIPTDIR}/k8stool image --source-tag "${imagename}"
          # Remove the cached image
          docker exec astra-control-plane crictl rmi "${anImage}" >/dev/null 2>&1 || true
      fi
    done
    kubectl --context ${CONTEXT} rollout restart -n ${NAMESPACE} deployment ${DEPLOYMENT}
  fi
}

function testHostpathSnapshot() {
  testns="testhostpath"
  # Create a namespace to test hostpath snapshot
  kubectl create namespace ${testns} --dry-run=client -o yaml \
    | kubectl apply -f -

  # Create persistent volume
  kubectl apply -n ${testns} -f /home/hostpath/test/csi-pvc.yaml
  sleep 1
  # Check pvc
  name=$(kubectl get -n ${testns} pvc -o jsonpath --template={.items[0].metadata.name})
  if [[ "${name}" == "csi-pvc" ]]; then
    echo -e "${Green}pvc created successfully${ColorOff}"
  else
    echo -e "${Red}pvc creation failed${ColorOff}"
  fi

  # Create volume snapshot
  kubectl apply -n ${testns} -f /home/hostpath/test/csi-snapshot-v1.yaml
  sleep 1

  # Check volume snapshot
  name=$(kubectl get -n ${testns} volumesnapshot -o jsonpath --template={.items[0].metadata.name})
  if [[ "${name}" == "new-snapshot-demo" ]]; then
    echo -e "${Green}snapshot created successfully${ColorOff}"
  else
    echo -e "${Red}snapshot creation failed${ColorOff}"
  fi

  # Check volume snapshot content
  name=$(kubectl get -n ${testns} volumesnapshotcontent -o jsonpath --template={.items[0].status.readyToUse})
  if [[ "${name}" == "true" ]]; then
    echo -e "${Green}snapshot content is ready${ColorOff}"
  else
    echo -e "${Red}snapshot content was not ready${ColorOff}"
  fi

  # Now restore the snapshot
  kubectl apply -n ${testns} -f /home/hostpath/test/csi-restore.yaml
  sleep 1
  # There should be two pvcs
  # Use this to get all the names
  #    name=$(kubectl get -n ${testns} pvc -o jsonpath --template={.items[*].metadata.name})
  # Get the 2nd pvc name
  name=$(kubectl get -n ${testns} pvc -o jsonpath --template={.items[1].metadata.name})
  if [[ ! -z ${name} ]]; then
    echo -e "${Green}pvc restore was successful${ColorOff}"
  else
    echo -e "${Red}pvc restore failed${ColorOff}"
  fi
}

function setupHostpath() {

  csins="hostpath"
  # Create a namespace to test hostpath snapshot
  kubectl create namespace ${csins} --dry-run=client -o yaml \
    | kubectl apply -f -

  /home/hostpath/deploy.sh

  # Create storage class
  kubectl apply -f /home/hostpath/storageclass/csi-storageclass.yaml
}

function setupTrident() {
  set +e
  tridentns="trident"
  kubectl get namespace ${tridentns} --context "${CONTEXT}" >/dev/null 2>&1
  # namespace does not exist, create it
  if [[ $? == 1 ]]; then
    echo "Creating namespace ${tridentns}"
    set -e
    kubectl create ns ${tridentns} --context "${CONTEXT}"
  else
    echo "Namespace ${tridentns} already exists"
  fi
  set -e

  # Getting trident images
  tridentImages=("netapp/trident:23.07.0" "netapp/trident-autosupport:23.07.0" "netapp/trident-operator:23.07.0" \
    "registry.k8s.io/sig-storage/csi-provisioner:v3.4.0" "registry.k8s.io/sig-storage/csi-attacher:v4.1.0" \
    "registry.k8s.io/sig-storage/csi-resizer:v1.7.0" "registry.k8s.io/sig-storage/csi-snapshotter:v3.0.3" \
    "registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.7.0")

  for item in ${tridentImages[@]}; do
    echo -e "${Green}Pulling image ${item}...${ColorOff}"
    docker pull --platform=linux/amd64 ${item}
    ${SCRIPTDIR}/k8stool image --load-or-push true --source-tag "${item}" --context ${CONTEXT}
  done

  echo -e "${Green}Deploying trident onto kubernetes cluster...${ColorOff}"
  kubectl --context "${CONTEXT}" apply -f /home/trident/tridentorchestrator.yaml -n ${tridentns}
  echo "Trident CRDs are deployed"
  ARCH=$(uname -m) && if [[ "${ARCH}" == "aarch64" ]]; then ARCH=arm64; fi
  if [[ "${ARCH}" == "x86_64" ]]; then ARCH="amd64"; fi

  # Now make sure that the node using right architecture node label
  ARCH=$ARCH yq e '.spec.controllerPluginNodeSelector."kubernetes.io/arch" = env(ARCH)' -i /home/trident/tridentorchestrator_cr.yaml
  ARCH=$ARCH yq e '.spec.nodePluginNodeSelector."kubernetes.io/arch" = env(ARCH)' -i /home/trident/tridentorchestrator_cr.yaml

  # Deploy the orchestrator
  kubectl --context ${CONTEXT} apply -f /home/trident/tridentorchestrator_cr.yaml -n ${tridentns}
}

function doAddon() {

  # This section deploys hashcorp vault onto the cluster
  echo -e "${Green}Deploying Hashicorp Vault...${ColorOff}"
  helm repo add hashicorp https://helm.releases.hashicorp.com >/dev/null 2>&1

  kubectl --context ${CONTEXT} create namespace vault --dry-run=client -o yaml \
    | kubectl --context ${CONTEXT} apply -f -

  # Try to uninstall it if there is one already there
  helm --kube-context ${CONTEXT} uninstall vault -n vault >/dev/nul 2>&1 || true
  sleep 3
  # Try to remove the pvc if there is one there
  kubectl --context ${CONTEXT} delete pvc -n vault data-vault-0 >/dev/nul 2>&1 || true
  sleep 3

  # Try to install
  helm --kube-context ${CONTEXT} install vault hashicorp/vault --namespace vault >/dev/null 2>&1 || true
  # Wait for vault to be ready to initialize
  while : ; do
    isready=$(kubectl --context ${CONTEXT} logs -n vault vault-0 2>/dev/null | grep 'seal configuration missing, not initialized' | head -n1)
    if [[ ! -z $isready ]]; then
      break
    fi
    echo 'Waiting for vault to be available...'
    sleep 3
  done

  kubectl --context ${CONTEXT} exec -ti -n vault statefulset/vault -- vault operator init --format json > /home/work/astra3/vault.key.json
  allkeys=($(cat /home/work/astra3/vault.key.json | jq -r '.unseal_keys_b64 | join(" ")'))
  for key in ${allkeys[@]}; do
    kubectl --context ${CONTEXT} exec -ti -n vault statefulset/vault -- vault operator unseal ${key} >/dev/null 2>&1
  done

  echo -e "${Green}Deploying MinIO...${ColorOff}"
  # This section setup minio onto the cluster
  kubectl --context ${CONTEXT} apply -n ${NAMESPACE} -f /home/addon/minio-dev.yaml

  # Wait for minio to be ready
  while : ; do
    isready=$(kubectl --context ${CONTEXT} logs -n ${NAMESPACE} minio 2>/dev/null | grep '1 Online, 0 Offline' | head -n1)
    if [[ ! -z $isready ]]; then
      break
    fi
    echo 'Waiting for minio to be available...'
    sleep 3
  done

  # Now we setup alias named minio and create bucket for test using the default userid and password
  kubectl --context ${CONTEXT} exec -ti -n ${NAMESPACE} pod/minio \
    -- chmod +x /opt/bin/mc
  kubectl --context ${CONTEXT} exec -ti -n ${NAMESPACE} pod/minio \
    -- mc config host add minio http://minio.neptune-system.svc:9000 minioadmin minioadmin

  # Create the test app bucket
  kubectl --context ${CONTEXT} exec -ti -n ${NAMESPACE} pod/minio \
    -- mc mb minio/test-appvault1 >/dev/nul 2>&1 || true
}

function setupTest() {
  workloadns=$1
  neptunens=$2
  echo -e "${Green}Creating workload namespace ${workloadns}...${ColorOff}"
  # Create a namespace for the workload to run
  kubectl create namespace ${workloadns} --dry-run=client -o yaml \
    | kubectl apply -f -

  # Now deploy the workload
  echo -e "${Green}Deploying workload...${ColorOff}"
  kubectl apply -n ${workloadns} -f /home/examples/workload.yaml

  # Now create a namespace to hold all the neptune resources
  echo -e "${Green}Creating namespace ${neptunens} for neptune resources...${ColorOff}"
  kubectl create namespace ${neptunens} --dry-run=client -o yaml \
    | kubectl apply -f -

  allfiles=(app-vault.yaml app.yaml snapshot.yaml backup.yaml)
  # fixup the app namespace attribute
  echo -e "${Green}Creating app for namespace ${workloadns}...${ColorOff}"
  ns="${workloadns}" yq '.spec.includedNamespaces[0].namespace = env(ns)' \
    -i /home/examples/app.yaml

  for afile in ${allfiles[@]}; do
    echo -e "${Green}Creating ${afile} in namespace ${neptunens}...${ColorOff}"
    kubectl apply -n ${neptunens} -f /home/examples/${afile}
  done
}

function removeTest() {
  workloadns=$1
  neptunens=$2
  # Now remove the workload
  kubectl delete -n ${workloadns} -f /home/examples/workload.yaml

  # delete the namespace for the workload
  kubectl delete namespace ${workloadns} >/dev/null 2>&1

  allfiles=(backup.yaml  snapshot.yaml app.yaml app-vault.yaml)
  for afile in ${allfiles[@]}; do
    kubectl delete -n ${neptunens} -f /home/examples/${afile}
  done
}


#=================Start main process=========================

set -e
checkEnvironmentVariables

# cluster processes
if [[ "${CMD}" == "up" ]]; then
  echo ""
  checkInNeptune

  echo -e "${Green}Setting up kubernetes cluster...${ColorOff}"
  ${SCRIPTDIR}/k8stool clusters --workdir "${WORKDIR}" --target-dir "${WORKDIR}"
  validateContext
  installStorageCRDs

  ${SCRIPTDIR}/k8stool cert --namespace ${NAMESPACE} --context ${CONTEXT} --workdir "${WORKDIR}"
  
  doNeptuneImage

  # Setup hostpath csi driver without test
  setupHostpath
  doAddon

  deployNeptune

elif [[ "${CMD}" == "prepare" ]]; then
  echo ""
  checkInNeptune

  echo ""
  echo -e "${Green}Setting up kubernetes cluster...${ColorOff}"
  ${SCRIPTDIR}/k8stool clusters --workdir "${WORKDIR}" --target-dir "${WORKDIR}"
  validateContext
  installStorageCRDs

  validateContext

  # echo -e "${Green}Create traefik certificate${ColorOff}"
  ${SCRIPTDIR}/k8stool cert --namespace ${NAMESPACE} --context ${CONTEXT} --workdir "${WORKDIR}"

  echo -e "${Green}Ready to deploy Astra Neptune, please run deploy command${ColorOff}"

elif [[ "${CMD}" == "down" ]]; then
  echo ""
  echo -e "${Green}Removing Neptune CRDs and Controller...${ColorOff}"
  checkInNeptune

  make undeploy
  make uninstall

elif [[ "${CMD}" == "clean" ]]; then
  echo -e "${Green}Removing Astra and cluster...${ColorOff}"
  ${SCRIPTDIR}/k8stool cluster -d

elif [[ "${CMD}" == "cleanall" ]]; then
  echo -e "${Green}Removing everything...${ColorOff}"
  ${SCRIPTDIR}/k8stool cluster -d
  # Remove the proxy and the local registry as well
  docker rm -f kubeproxy &> /dev/null || true
  docker rm -f kind-registry &> /dev/null || true
  # Need to clean up docker volumes
  docker volume prune -f &> /dev/null || true

elif [[ "${CMD}" == "deploy" ]]; then
  checkInNeptune
  validateContext
  # Always check and recreate namespace and the cert in case it was removed
  ${SCRIPTDIR}/k8stool cert --namespace ${NAMESPACE} --context ${CONTEXT} --workdir "${WORKDIR}"
  deployNeptune

elif [[ "${CMD}" == "refresh" ]]; then
  checkInNeptune
  validateContext
  refreshDeployment

elif [[ "${CMD}" == "image" ]]; then
  doNeptuneImage

elif [[ "${CMD}" == "update" ]]; then
  doADTUpdate

elif [[ "${CMD}" == "addon" ]]; then
  validateContext
  doAddon

elif [[ "${CMD}" == "trident" ]]; then
  validateContext
  setupTrident

elif [[ "${CMD}" == "hostpath" ]]; then
  validateContext
  setupHostpath

elif [[ "${CMD}" == "testhostpath" ]]; then
  validateContext
  testHostpathSnapshot

elif [[ "${CMD}" == "test" ]]; then
  validateContext
  setupTest test1 neptune-system

elif [[ "${CMD}" == "rmtest" ]]; then
  validateContext
  removeTest test1 neptune-system

elif [[ "${CMD}" == "make" ]]; then
  checkInNeptune

  if [[ "$REST" == "all-buildx" ]]; then

    # Only login to image repository when both registry userid and token are available
    # this is to allow publish to the image repo will be successful
    if [[ ! -z $REGISTRY_USERID ]] && [[ ! -z $REGISTRY_TOKEN ]]; then
      # Login to the registry only to allow multiarch image built and push easier
      if [[ -z "$REGISTRY" ]]; then
        echo ${REGISTRY_TOKEN} | docker login docker.io -u ${REGISTRY_USERID} --password-stdin >/dev/null 2>&1
        REGISTRY="${REGISTRY_USERID}/"
      else
        echo ${REGISTRY_TOKEN} | docker login ${REGISTRY} -u ${REGISTRY_USERID} --password-stdin >/dev/null 2>&1
      fi
    fi
  fi

  if [[ -z ${PLATFORMS} ]]; then
    ARCH=$(uname -m)
    if [[ "${ARCH}" == "aarch64" ]]; then ARCH=arm64; fi
    if [[ "${ARCH}" == "x86_64" ]]; then ARCH="amd64"; fi
    PLATFORMS=linux/${ARCH}
  fi

  # We are ready to execute,
  make ${REST} REGISTRY=${REGISTRY} PLATFORMS=${PLATFORMS} REPOSITORY=${REPOSITORY} TAG=${TAG}

elif [[ "${CMD}" == "bash" ]]; then
  checkInNeptune
  echo -e "${Green}Entering bash session...${ColorOff}"
  # env IMG=kind-registry:5001/controller:latest bash
  bash
fi
#=================End main process=========================
