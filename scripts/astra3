#!/bin/bash

ColorOff='\033[0m'        # Text Reset
Black='\033[0;30m'        # Black
Red='\033[0;31m'          # Red
Green='\033[0;32m'        # Green
SCRIPTDIR=$(dirname $0)
APPNAME=$(basename "$0")
CURDIR=$(pwd)
WORKDIR=${WORKDIR:-/tmp/astra3}
COMPONENT="all"
NAMESPACE="${NAMESPACE:-neptune-system}"
DEPLOYMENT="${DEPLOYMENT:-neptune-controller-manager}"
TAG=${TAG:-latest}
SECRETNAME=${SECRETNAME:-regcred}
APITOKEN=${APITOKEN:-""}
CLUSTERNAME=${CLUSTERNAME:-"neptune"}

function printHelp() {
  echo ""
  echo "Usage:"
  echo "    ${APPNAME} <cmd> [options]"
  echo ""
  echo -e "Available commands:"
  echo "     up       - Start up astra network include cluster"
  echo "     down     - Remove all astra but k8s cluster"
  echo "     image    - Update image to the local repo"
  echo "     prepare  - Get cluster and image ready"
  echo "     deploy   - Deploy astra components"
  echo "     clean    - Remove all astra including k8s cluster"
  echo "     cleanall - Remove all astra including k8s cluster, proxy, local registry"
  echo "     update   - update this tool"
  echo "     trident  - Set up trident"
  echo "     refresh  - restart a deployment and its image"
  echo "     webui    - expose minio and vault console"
  echo "     test     - run neptune tests"
  echo "     vtest    - verify test results"
  echo "     rmtest   - remove neptune tests"
  echo "     rstest   - test restore of a backup"
  echo "     make     - run neptune make command"
  echo "     conn     - deploy connector"
  echo "     bash     - run bash interactively"
  echo ""
  echo -e "Parameters for ${Green}up or prepare${ColorOff} command:"
  echo "       --cluster-name   - name of the k8s cluster to be created"
  echo ""
  echo -e "Parameters for ${Green}refresh${ColorOff} command:"
  echo "     -d|--deployment-name  - a deployment name such as neptune-controller-manager"
  echo "     -n|--namespace        - the namespace where the deployment is, default neptune-system"
  echo ""
  echo -e "Parameters for ${Green}deploy${ColorOff} command:"
  echo "     -a|--astra-account-id - astra account id"
  echo "     -t|--apitoken         - astra api token"
  echo "     -s|--secretname       - image pulling secret name"
  echo "     --bridgeurl           - astra endpoint"
  echo "     --aliasip             - astra alias ip address"
  echo ""
}

function validateCMD() {
  cmd=$1
  allCommands=("up" "down" "image" "deploy" "clean" "cleanall" "prepare" "refresh" "make" "webui" "conn" \
    "update" "k8stool" "bash" "addon" "trident" "hostpath" "test" "rmtest" "vtest" "rstest" "testhostpath")
  ccmd=""
  for item in "${allCommands[@]}"; do
    if [[ "${cmd}" == "${item}" ]]; then
      ccmd="${cmd}"
      isValidCMD="true"
      break
    fi
  done
  if [[ -z "${ccmd}" ]]; then
    if [[ "${cmd}" != "-h" ]] && [[ "${cmd}" != "--help" ]] && [[ "${cmd}" != "" ]]; then
      echo ""
      echo -e "ERROR: ${Red}${cmd}${ColorOff} is not a supported command!"
      printHelp "${isValidCMD}"
      exit 1
    else
      printHelp "${isValidCMD}"
      exit 0
    fi
  fi
}

CMD=$1
shift
# This saves the rest of the command in case it is to pass along for make command
REST="$@"
# Validate the command
validateCMD "${CMD}"

# We will only handle command parameters if command was not make and not k8stool
if [[ "${CMD}" == "make" || "${CMD}" == "k8stool" || "${CMD}" == "bash" ]]; then
  echo ""
else
  # Handling parameters
  while [[ $# -gt 0 ]]; do
    optkey="$1"
    case $optkey in
      -h|--help)
        printHelp "true"; exit 0;;
      --targetports)
        TARGETPORTS="$2";shift 2;;
      -n|--namespace)
        NAMESPACE="$2";shift 2;;
      -w|--workdir)
        WORKDIR="$2";shift 2;;
      -a|--astra-account-id)
        ACCOUNTID="$2";shift 2;;
      -d|--deployment-name)
        DEPLOYMENT="$2";shift 2;;
      -s|--secretname)
        SECRETNAME="$2";shift 2;;
      -t|--apitoken)
        APITOKEN="$2";shift 2;;
      --cluster-name)
        CLUSTERNAME="$2";shift 2;;
      --bridgeurl)
        BRIDGEURL="$2";shift 2;;
      --aliasip)
        ALIASIP="$2";shift 2;;
      *) # unknown option
        echo "parameter $1 is not supported"; exit 1;;
    esac
  done
fi

function setupContext() {
  # Always use the ${CLUSTERNAME}.yaml as the kubeconfig file
  export KUBECONFIG=${HOME}/.kube/${CLUSTERNAME}.yaml
}

function installStorageCRDs() {
  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.2.1/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml
  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.2.1/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml
  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.2.1/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml
  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.2.1/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml
  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.2.1/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml
}

function doADTUpdate() {
  adtimage=$(docker inspect astra3 --format "{{.Config.Image}}" 2>/dev/null || true)
  if [[ ! -z "${adtimage}" ]]; then
    if [[ ! -z $REGISTRY_USERID ]] && [[ ! -z $REGISTRY_TOKEN ]]; then
      if [[ -z $REGISTRY ]]; then
        echo ${REGISTRY_TOKEN} | docker login docker.io -u ${REGISTRY_USERID} --password-stdin >/dev/null 2>&1
      else
        echo ${REGISTRY_TOKEN} | docker login $REGISTRY -u ${REGISTRY_USERID} --password-stdin >/dev/null 2>&1
      fi
    fi
    # Pull the image
    docker pull "${adtimage}"
    # Try to clean up dangling images, use a variable to avoid none 0 return
    notuseval=$(docker rmi -f $(docker images -f "dangling=true") >/dev/null 2>&1 || true)
  fi
}

function loginToDockerHub() {
  if [[ ! -z "${GITHUB_ID}" ]] && [[ ! -z "${GITHUB_TOKEN}" ]]; then
    echo -e "${Green}Logging into docker hub...${ColorOff}"
    echo $DH_TOKEN | docker login -u $DH_ID --password-stdin docker.io >/dev/null 2>&1
  fi
}

function checkInNeptune() {
    projectName=$(cat PROJECT | yq '.projectName')
    if [[ ! -f "${CURDIR}/Makefile" ]] || [[ "${projectName}" != "neptune" ]]; then
      echo -e "${Red}You are not in neptune root directory, quiting...${ColorOff}"
      exit 1
    fi
    chown $(id -u):$(id -g) $HOME/.kube
}

function checkEnvironmentVariables() {
  envs=(REGISTRY REGISTRY_USERID REGISTRY_TOKEN TAG PLATFORMS)
  for value in ${envs[@]}; do
    if [[ "${value}" == "REGISTRY_TOKEN" ]]; then
      echo -e "${Green}${value}${ColorOff}=************"
    else
      echo -e "${Green}${value}${ColorOff}=${!value}"
    fi
  done
  echo ""
}

function wait_for_pod() {
  namespace=$1
  podlabel=$2
  condition=$3
  while : ; do
    waitresult=$(kubectl wait pod -n ${namespace} --for=condition=${condition} \
      -l "${podlabel}" --timeout=60s 2>/dev/null || true)
    # If the wait returns, it can only be either timed out or condition met. We check if there is
    # timedout, we continue to wait. Since the timeout is an error, and it goes to /dev/null, only
    # condition met will have anything in waitresult, in any other condition, the waitresult is empty.
    if [[ -z "${waitresult}" ]]; then
      currentTime=$(date +%s)
      elapsed=$(( currentTime - startTime ))
      echo -ne "\033[0K\r"
      echo -ne "   Time elapsed: ${Green}${elapsed}${ColorOff} seconds\033[0K"
    else
       break
    fi
  done
  echo ""
  echo -e "${Green}Astra pods are ready now${ColorOff}"
}

function deployConnector() {
  # Only deploy connector when apitoken is provided.
  if [[ ! -z "${APITOKEN}" ]]; then
    #1. deploy the connector operator and wait for it to become ready
    kubectl apply -f https://github.com/NetApp/astra-connector-operator/releases/latest/download/astraconnector_operator.yaml
    wait_for_pod 'astra-connector-operator' 'control-plane=controller-manager' 'ready'

    #2. Create secret in neptune-system namespace
    exists=$(kubectl get -n ${NAMESPACE} secrets astra-token --no-headers 2>/dev/null || true)
    if [[ ! -z ${exists} ]]; then
      kubectl delete -n ${NAMESPACE} secrets astra-token >/dev/null 2>&1 || null
    fi
    kubectl create secret generic astra-token \
      --from-literal=apiToken=${APITOKEN} -n ${NAMESPACE} || true

    #3. Now create connector CR.
    if [[ ! -z "${ACCOUNTID}" ]] && [[ ! -z "${ALIASIP}" ]]; then
      ACCOUNTID=$ACCOUNTID yq e '.spec.astra.accountId = env(ACCOUNTID)' -i /home/addon/connector.yaml
      ALIASIP=$ALIASIP yq e '.spec.natsSyncClient.hostAliasIP = env(ALIASIP)' -i /home/addon/connector.yaml

      if [[ -z "${BRIDGEURL}" ]]; then
        BRIDGEURL="https://integration.astra.netapp.io"
      fi
      BRIDGEURL=$BRIDGEURL yq e '.spec.natsSyncClient.cloudBridgeURL = env(BRIDGEURL)' -i /home/addon/connector.yaml

      if [[ ! -z "${REGISTRY}" ]]; then
        REGISTRY="${REGISTRY}" yq e '.spec.imageRegistry.name = env(REGISTRY)' -i /home/addon/connector.yaml
      else
        REGISTRY="kind-registry:5001" yq e '.spec.imageRegistry.name = env(REGISTRY)' -i /home/addon/connector.yaml
      fi
      if [[ ! -z "${SECRETNAME}" ]]; then
        SECRETNAME="${SECRETNAME}" yq e '.spec.imageRegistry.secret = env(SECRETNAME)' -i /home/addon/connector.yaml
      fi
      if [[ ! -z "${CLUSTERNAME}" ]]; then
        CLUSTERNAME="${CLUSTERNAME}" yq e '.spec.astra.clusterName = env(CLUSTERNAME)' -i /home/addon/connector.yaml
      fi

      # Create the connector CR
      kubectl -n neptune-system apply -f /home/addon/connector.yaml
    fi
  fi
}

function deployNeptune() {
  echo -e "${Green}Deploying Neptune and Connector...${ColorOff}"
  echo -e "Target Kubernetes onto cluster: ${Green}${CLUSTERNAME}${ColorOff}" 
  if [[ -z ${REGISTRY} ]]; then
    IMG="kind-registry:5001/controller:${TAG}" REGISTRY="kind-registry:5001" TAG=${TAG} make deploy
  else
    # fix up registry missing trailing slash
    reg=$(echo ${REGISTRY%/})
    SECRET=${SECRETNAME} REGISTRY="${reg}/" TAG=${TAG} make deploy
  fi
  deployConnector

  echo -e "${Green}Neptune is ready!!!${ColorOff}"
}

function loginToRepo() {
  reg=$1
  # the login file should be one line with id:token, expect the
  # file login be in /tmp/astra3/ directory for this to work
  if [[ -f /home/work/astra3/login ]]; then
    keyPair=$(cat /home/work/astra3/login)
    id=$(echo $keyPair | cut -d ":" -f 1)
    pw=$(echo $keyPair | cut -d ":" -f 2)
    echo ${pw} | docker login ${reg} -u ${id} --password-stdin >/dev/null 2>&1 || true
  fi
}

function doNeptuneImage() {
  # The assumption is that this repo has all necessary neptune needed images
  echo -e "${Green}Setting up Neptune Manager image...${ColorOff}"

  prefix="netappdownloads.jfrog.io/docker-astra-control-staging/arch30/neptune/"
  if [[ ! -f /home/work/astra3/images ]]; then
    cp /home/addon/neptune-images.list /home/work/astra3/neptune-images.list
  fi

  # this image list file should only contain the image name and tag.
  filecontent=$(cat /home/work/astra3/neptune-images.list)
  allimages=($(echo $filecontent))

  loggedIn="false"
  for img in ${allimages[@]}; do
    img=$(echo "${img%:}")  # this is to remove the possible trailing colon
    if [[ -z "${img}" ]]; then
      continue
    fi
    # Get the fullname of the image
    fullname=$(echo $img | cut -d ":" -f 1)
    # Get the name of the image
    name=$(echo ${fullname##*/})
    #handle the tag
    tag=$(echo $img | cut -d ":" -f 2)
    if [[ -z ${tag} ]] || [[ "${tag}" == "${fullname}" ]]; then
      tag=${TAG:-latest}
    fi
    # check if the image is already available locally
    localImg=$(docker image ls ${name}:${tag} --quiet)
    if [[ -z "${localImg}" ]]; then
      # Since the local image does not exist, we will try to retrieve it from jfrog
      if [[ "${loggedIn}" == "false" ]]; then
        loginToRepo $prefix
        loggedIn="true"
      fi
      echo -e "${Red}${name}:${tag} local image does not exist, try to pull from ${prefix}${ColorOff}"
      docker pull ${prefix}${name}:${tag} 
      docker tag ${prefix}${name}:${tag} ${name}:${tag}
      docker tag ${prefix}${name}:${tag} localhost:5001/${name}:${tag}
    else
      # local image exists, we tag them to the localhost one, then push
      docker tag ${name}:${tag} localhost:5001/${name}:${tag}
    fi
    docker push localhost:5001/${name}:${tag}
  done
}

function refreshDeployment() {
  if [[ "${DEPLOYMENT}" == "" ]]; then
    DEPLOYMENT="neptune-controller-manager"
  fi

  OLDIFS=$IFS
  dep=$(kubectl get -n ${NAMESPACE} deployment "${DEPLOYMENT}" -o \
    jsonpath='{.spec.template.spec.containers[*].image}' 2>/dev/null||true)
  if [[ "${dep}" == "" ]]; then
    echo -e "${Red}Could not find the deployment ${DEPLOYMENT}${ColorOff}"
    echo -e "${Green}If the deployment was not installed in neptune-system, you can use -n to specify${ColorOff}"
    IFS=$OLDIFS
    exit 1
  else
    # Now need to get the tag and the actual image name
    for anImage in ${dep[@]}; do
      IFS=$'/' parts=($(echo "$anImage"|rev))
      imagename=$(echo "${parts[0]}"|rev)
      IFS=$OLDIFS
      if [[ "${imagename}" != "" ]] && [[ "${anImage}" =~ "kind-registry:5001" ]]; then
          echo -e "Ready to update ${Green}${imagename}${ColorOff}"
          # astra image, push the newer image
          ${SCRIPTDIR}/k8stool image --source-tag "${imagename}"
          # Remove the cached image
          docker exec ${CLUSTERNAME}-control-plane crictl rmi "${anImage}" >/dev/null 2>&1 || true
      fi
    done
    kubectl rollout restart -n ${NAMESPACE} deployment ${DEPLOYMENT}
  fi
}

function testHostpathSnapshot() {
  testns="testhostpath"
  # Create a namespace to test hostpath snapshot
  kubectl create namespace ${testns} --dry-run=client -o yaml \
    | kubectl apply -f -

  # Create persistent volume
  kubectl apply -n ${testns} -f /home/hostpath/test/csi-pvc.yaml
  sleep 1
  # Check pvc
  name=$(kubectl get -n ${testns} pvc -o jsonpath --template={.items[0].metadata.name})
  if [[ "${name}" == "csi-pvc" ]]; then
    echo -e "${Green}pvc created successfully${ColorOff}"
  else
    echo -e "${Red}pvc creation failed${ColorOff}"
  fi

  # Create volume snapshot
  kubectl apply -n ${testns} -f /home/hostpath/test/csi-snapshot-v1.yaml
  sleep 1

  # Check volume snapshot
  name=$(kubectl get -n ${testns} volumesnapshot -o jsonpath --template={.items[0].metadata.name})
  if [[ "${name}" == "new-snapshot-demo" ]]; then
    echo -e "${Green}snapshot created successfully${ColorOff}"
  else
    echo -e "${Red}snapshot creation failed${ColorOff}"
  fi

  # Check volume snapshot content
  name=$(kubectl get -n ${testns} volumesnapshotcontent -o jsonpath --template={.items[0].status.readyToUse})
  if [[ "${name}" == "true" ]]; then
    echo -e "${Green}snapshot content is ready${ColorOff}"
  else
    echo -e "${Red}snapshot content was not ready${ColorOff}"
  fi

  # Now restore the snapshot
  kubectl apply -n ${testns} -f /home/hostpath/test/csi-restore.yaml
  sleep 1
  # There should be two pvcs
  # Use this to get all the names
  #    name=$(kubectl get -n ${testns} pvc -o jsonpath --template={.items[*].metadata.name})
  # Get the 2nd pvc name
  name=$(kubectl get -n ${testns} pvc -o jsonpath --template={.items[1].metadata.name})
  if [[ ! -z ${name} ]]; then
    echo -e "${Green}pvc restore was successful${ColorOff}"
  else
    echo -e "${Red}pvc restore failed${ColorOff}"
  fi
}

function setupHostpath() {

  csins="hostpath"
  # Create a namespace to test hostpath snapshot
  kubectl create namespace ${csins} --dry-run=client -o yaml \
    | kubectl apply -f -

  /home/hostpath/deploy.sh

  # Create storage class
  kubectl apply -f /home/hostpath/storageclass/csi-storageclass.yaml

  # Set the standard not to be the default sc
  kubectl patch storageclass standard --type=merge -p \
   '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'

  # Set the hostpath to be the default
  kubectl patch storageclass csi-hostpath-sc --type=merge -p \
   '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

  # Set the default snapshot sc for volumesnapshot class
  kubectl patch volumesnapshotclass csi-hostpath-snapclass --type=merge -p \
   '{"metadata": {"annotations":{"snapshot.storage.kubernetes.io/is-default-class":"true"}}}'
}

function setupTrident() {
  set +e
  tridentns="trident"
  kubectl get namespace ${tridentns} >/dev/null 2>&1
  # namespace does not exist, create it
  if [[ $? == 1 ]]; then
    echo "Creating namespace ${tridentns}"
    set -e
    kubectl create ns ${tridentns}
  else
    echo "Namespace ${tridentns} already exists"
  fi
  set -e

  # Getting trident images
  tridentImages=("netapp/trident:23.07.0" "netapp/trident-autosupport:23.07.0" "netapp/trident-operator:23.07.0" \
    "registry.k8s.io/sig-storage/csi-provisioner:v3.4.0" "registry.k8s.io/sig-storage/csi-attacher:v4.1.0" \
    "registry.k8s.io/sig-storage/csi-resizer:v1.7.0" "registry.k8s.io/sig-storage/csi-snapshotter:v3.0.3" \
    "registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.7.0")

  for item in ${tridentImages[@]}; do
    echo -e "${Green}Pulling image ${item}...${ColorOff}"
    docker pull --platform=linux/amd64 ${item}
    ${SCRIPTDIR}/k8stool image --load-or-push true --source-tag "${item}"
  done

  echo -e "${Green}Deploying trident onto kubernetes cluster...${ColorOff}"
  kubectl apply -f /home/trident/tridentorchestrator.yaml -n ${tridentns}
  echo "Trident CRDs are deployed"
  ARCH=$(uname -m) && if [[ "${ARCH}" == "aarch64" ]]; then ARCH=arm64; fi
  if [[ "${ARCH}" == "x86_64" ]]; then ARCH="amd64"; fi

  # Now make sure that the node using right architecture node label
  ARCH=$ARCH yq e '.spec.controllerPluginNodeSelector."kubernetes.io/arch" = env(ARCH)' -i /home/trident/tridentorchestrator_cr.yaml
  ARCH=$ARCH yq e '.spec.nodePluginNodeSelector."kubernetes.io/arch" = env(ARCH)' -i /home/trident/tridentorchestrator_cr.yaml

  # Deploy the orchestrator
  kubectl apply -f /home/trident/tridentorchestrator_cr.yaml -n ${tridentns}
}

function deployVault() {
  # This section deploys hashcorp vault onto the cluster
  echo -e "${Green}Deploying Hashicorp Vault...${ColorOff}"
  helm repo add hashicorp https://helm.releases.hashicorp.com >/dev/null 2>&1

  kubectl create namespace vault --dry-run=client -o yaml \
    | kubectl apply -f -

  # Try to uninstall it if there is one already there
  helm uninstall vault -n vault >/dev/nul 2>&1 || true
  sleep 3
  # Try to remove the pvc if there is one there
  kubectl delete pvc -n vault data-vault-0 >/dev/nul 2>&1 || true
  sleep 3

  # Try to install
  helm install vault hashicorp/vault --namespace vault --set ui.enable=true >/dev/null 2>&1 || true
  # Wait for vault to be ready to initialize
  while : ; do
    isready=$(kubectl logs -n vault vault-0 2>/dev/null | grep 'seal configuration missing, not initialized' | head -n1)
    if [[ ! -z $isready ]]; then
      break
    fi
    echo 'Waiting for vault to be available...'
    sleep 3
  done

  kubectl exec -ti -n vault statefulset/vault -- vault operator init --format json > /home/work/astra3/vault.key.json
  allkeys=($(cat /home/work/astra3/vault.key.json | jq -r '.unseal_keys_b64 | join(" ")'))
  for key in ${allkeys[@]}; do
    kubectl exec -ti -n vault statefulset/vault -- vault operator unseal ${key} >/dev/null 2>&1
  done
  kubectl apply -n vault -f /home/addon/vault-lb.yaml

  # Get root token
  # roottoken=$(cat /home/work/astra3/vault.key.json | jq -r '.root_token')
  # Here we use the root token login first, then enable username password login,then create a admin user with root access
  # kubectl exec -n vault statefulset/vault -- \
  #  /bin/sh -c "echo ${roottoken} | vault login -; vault auth enable userpass; vault write auth/userpass/users/admin policies=root password=admin;"

}

function deployMinio() {
  echo -e "${Green}Deploying MinIO...${ColorOff}"
  # This section setup minio onto the cluster
  kubectl apply -n ${NAMESPACE} -f /home/addon/minio-dev.yaml
  kubectl apply -n ${NAMESPACE} -f /home/addon/minio-lb.yaml

  # Wait for minio to be ready
  while : ; do
    isready=$(kubectl logs -n ${NAMESPACE} minio 2>/dev/null | grep '1 Online, 0 Offline' | head -n1)
    if [[ ! -z $isready ]]; then
      break
    fi
    echo 'Waiting for minio to be available...'
    sleep 3
  done

  # Now we setup alias named minio and create bucket for test using the default userid and password
  kubectl exec -ti -n ${NAMESPACE} pod/minio \
    -- mc config host add minio http://minio.neptune-system.svc:9000 minioadmin minioadmin

  # Create the test app bucket
  kubectl exec -ti -n ${NAMESPACE} pod/minio \
    -- mc mb minio/test-appvault1 >/dev/nul 2>&1 || true
}

function doAddon() {
  deployMinio
  # Now setup the pathecho for test post summary
  kubectl -n ${NAMESPACE} apply -f /home/addon/pathecho-tls.yaml
}

function exposeUI() {
  hostWorkDir=$(docker inspect astra3 | jq -r '.[].Mounts | .[] |select( .Destination == "/home/work/astra3") | .Source')
  export HOSTWORKDIR=${hostWorkDir}
  # If only expose one service
  # ${SCRIPTDIR}/k8stool proxy --workdir "/home/work/astra3" --targetports "minio-ui 9090"
  ${SCRIPTDIR}/k8stool proxy --workdir "/home/work/astra3"
}

function isClusterOCP() {
  res=$(kubectl get crd securitycontextconstraints.security.openshift.io --no-headers 2>/dev/null)
  echo ${res}
}

function setupTest() {
  workloadns=$1
  neptunens=$2
  targetns=$3

  echo -e "${Green}Creating workload namespace ${workloadns}...${ColorOff}"
  # Create a namespace for the workload to run
  kubectl create namespace ${workloadns} --dry-run=client -o yaml \
    | kubectl apply -f -

  # Now deploy the workload
  echo -e "${Green}Deploying workload...${ColorOff}"
  isOCP=$(isClusterOCP) # empty means not OCP, non empty means OCP
  if [[ -z ${isOCP} ]]; then
    kubectl apply -n ${workloadns} -f /home/examples/workload.yaml
  else
    ns="${workloadns}" yq 'select(.metadata.name == "test-scc-binding").subjects[0].namespace = env(ns)' -i /home/examples/ocpworkload.yaml
    kubectl apply -n ${workloadns} -f /home/examples/ocpworkload.yaml
  fi

  # Now create a namespace to hold all the neptune resources
  echo -e "${Green}Creating namespace ${neptunens} for neptune resources...${ColorOff}"
  kubectl create namespace ${neptunens} --dry-run=client -o yaml \
    | kubectl apply -f -

  allfiles=(app-vault.yaml app.yaml snapshot.yaml backup.yaml resoucesummaryupload.yaml)
  # fixup the app namespace attribute
  echo -e "${Green}Creating app for namespace ${workloadns}...${ColorOff}"
  ns="${workloadns}" yq '.spec.includedNamespaces[0].namespace = env(ns)' \
    -i /home/examples/app.yaml

  for afile in ${allfiles[@]}; do
    echo -e "${Green}Creating ${afile} in namespace ${neptunens}...${ColorOff}"
    kubectl apply -n ${neptunens} -f /home/examples/${afile}
  done
}

function removeTest() {
  workloadns=$1
  neptunens=$2

  # Now remove the workload
  isOCP=$(isClusterOCP) # empty means not OCP, non empty means OCP
  if [[ -z ${isOCP} ]]; then
    kubectl delete -n ${workloadns} -f /home/examples/workload.yaml
  else
    ns="${workloadns}" yq 'select(.metadata.name == "test-scc-binding").subjects[0].namespace = env(ns)' -i /home/examples/ocpworkload.yaml
    kubectl delete -n ${workloadns} -f /home/examples/ocpworkload.yaml
  fi
  # delete the namespace for the workload
  kubectl delete namespace ${workloadns} >/dev/null 2>&1

  allfiles=(resoucesummaryupload.yaml backup.yaml snapshot.yaml app.yaml app-vault.yaml)
  for afile in ${allfiles[@]}; do
    kubectl delete -n ${neptunens} -f /home/examples/${afile} || true
  done

}

function verifyTest() {
  workloadns=$1
  neptunens=$2

  source /home/bin/verify
  verifyTests "${workloadns}" "${neptunens}"
}

function restoreTest() {
  workloadns=$1
  neptunens=$2
  targetns=$3
  backupname="test-backup1"

  appArchivePath=$(kubectl -n ${neptunens} get backups -o jsonpath='{range .items[?(@.metadata.name == "'$backupname'")]}{.status.appArchivePath}{end}')
  if [[ -z "${appArchivePath}" ]]; then
    echo -e "${Red}Cannot find the backup ${backupname}${ColorOff}"
    exit 1
  fi

  SNS=$appArchivePath yq e '.spec.appArchivePath = env(SNS)' -i /home/examples/backuprestore.yaml
  SNS=$workloadns yq e '.spec.namespaceMapping[0].source = env(SNS)' -i /home/examples/backuprestore.yaml
  SNS=$targetns yq e '.spec.namespaceMapping[0].target = env(SNS)' -i /home/examples/backuprestore.yaml

  echo -e "${Green}Ready to create the restore CR${ColorOff}"
  cat /home/examples/backuprestore.yaml
  # Now create this backup restore CR
  kubectl apply -n ${neptunens} -f /home/examples/backuprestore.yaml

}

#=================Start main process=========================

set -e
checkEnvironmentVariables
setupContext
# cluster processes
if [[ "${CMD}" == "up" ]]; then
  echo ""
  checkInNeptune

  echo -e "${Green}Setting up kubernetes cluster...${ColorOff}"
  ${SCRIPTDIR}/k8stool clusters --workdir "${WORKDIR}" --cluster-name "${CLUSTERNAME}"
  
  installStorageCRDs

  ${SCRIPTDIR}/k8stool cert --namespace ${NAMESPACE} --workdir "${WORKDIR}" --cluster-name "${CLUSTERNAME}"
  
  doNeptuneImage

  # Setup hostpath csi driver without test
  doAddon
  setupHostpath

  deployNeptune

elif [[ "${CMD}" == "prepare" ]]; then
  echo ""
  checkInNeptune

  echo ""
  echo -e "${Green}Setting up kubernetes cluster...${ColorOff}"
  ${SCRIPTDIR}/k8stool clusters --workdir "${WORKDIR}" --cluster-name ${CLUSTERNAME}

  installStorageCRDs

  # echo -e "${Green}Create traefik certificate${ColorOff}"
  ${SCRIPTDIR}/k8stool cert --namespace ${NAMESPACE} --workdir "${WORKDIR}"

  echo -e "${Green}Ready to deploy Astra Neptune, please run deploy command${ColorOff}"

elif [[ "${CMD}" == "down" ]]; then
  echo ""
  echo -e "${Green}Removing Neptune CRDs and Controller...${ColorOff}"
  checkInNeptune

  make undeploy
  make uninstall

elif [[ "${CMD}" == "clean" ]]; then
  echo -e "${Green}Removing Neptune and cluster...${ColorOff}"
  ${SCRIPTDIR}/k8stool cluster -d --cluster-name ${CLUSTERNAME}
  rm -rf ${HOME}/.kube/${CLUSTERNAME}.yaml

elif [[ "${CMD}" == "cleanall" ]]; then
  echo -e "${Green}Removing everything...${ColorOff}"
  ${SCRIPTDIR}/k8stool cluster -d --cluster-name ${CLUSTERNAME}
  rm -rf ${HOME}/.kube/${CLUSTERNAME}.yaml
  # Remove the proxy and the local registry as well
  docker rm -f kubeproxyp &> /dev/null || true
  docker rm -f kind-registry &> /dev/null || true
  # Need to clean up docker volumes
  docker volume prune -f &> /dev/null || true

elif [[ "${CMD}" == "deploy" ]]; then
  checkInNeptune
  
  # Always check and recreate namespace and the cert in case it was removed
  ${SCRIPTDIR}/k8stool cert --namespace ${NAMESPACE} --workdir "${WORKDIR}"
  deployNeptune

elif [[ "${CMD}" == "refresh" ]]; then
  checkInNeptune
  refreshDeployment

elif [[ "${CMD}" == "image" ]]; then
  doNeptuneImage

elif [[ "${CMD}" == "update" ]]; then
  doADTUpdate

elif [[ "${CMD}" == "addon" ]]; then
  doAddon

elif [[ "${CMD}" == "trident" ]]; then
  setupTrident

elif [[ "${CMD}" == "hostpath" ]]; then
  setupHostpath

elif [[ "${CMD}" == "testhostpath" ]]; then
  testHostpathSnapshot

elif [[ "${CMD}" == "test" ]]; then
  setupTest test1 neptune-system

elif [[ "${CMD}" == "vtest" ]]; then
  verifyTest test1 neptune-system

elif [[ "${CMD}" == "rmtest" ]]; then
  removeTest test1 neptune-system

elif [[ "${CMD}" == "rstest" ]]; then
  restoreTest test1 neptune-system test2

elif [[ "${CMD}" == "webui" ]]; then
  exposeUI

elif [[ "${CMD}" == "conn" ]]; then
  deployConnector

elif [[ "${CMD}" == "make" ]]; then
  checkInNeptune

  if [[ "$REST" == "all-buildx" ]]; then

    # Only login to image repository when both registry userid and token are available
    # this is to allow publish to the image repo will be successful
    if [[ ! -z $REGISTRY_USERID ]] && [[ ! -z $REGISTRY_TOKEN ]]; then
      # Login to the registry only to allow multiarch image built and push easier
      if [[ -z "$REGISTRY" ]]; then
        echo ${REGISTRY_TOKEN} | docker login docker.io -u ${REGISTRY_USERID} --password-stdin >/dev/null 2>&1
        REGISTRY="${REGISTRY_USERID}/"
      else
        echo ${REGISTRY_TOKEN} | docker login ${REGISTRY} -u ${REGISTRY_USERID} --password-stdin >/dev/null 2>&1
      fi
    fi
  fi

  if [[ -z ${PLATFORMS} ]]; then
    ARCH=$(uname -m)
    if [[ "${ARCH}" == "aarch64" ]]; then ARCH=arm64; fi
    if [[ "${ARCH}" == "x86_64" ]]; then ARCH="amd64"; fi
    PLATFORMS=linux/${ARCH}
  fi

  # We are ready to execute,
  make ${REST} REGISTRY=${REGISTRY} PLATFORMS=${PLATFORMS} REPOSITORY=${REPOSITORY} TAG=${TAG}

elif [[ "${CMD}" == "bash" ]]; then
  checkInNeptune
  echo -e "${Green}Entering bash session...${ColorOff}"
  # env IMG=kind-registry:5001/controller:latest bash
  bash
fi
#=================End main process=========================
